# SOSP 2024 Experiments

This document describes how to run the main experiments in the [SOSP '24](https://sigops.org/s/conferences/sosp/2024/) paper [Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving](https://arxiv.org/abs/2312.05385).

## Setup

### Hardware Setup

We have tested Apparate on CPU nodes on Cloudlab. 

> Artifact evaluators: Hello! Right now, please resort to your own compute resources for reproducing our results -- in theory, any Linux machine should be able to run our code. We will provide more instructions on accessing a Cloudlab node soon.

### Software Dependencies

For ease of reproduction, we use conda to create a virtual environment (Miniconda can be installed by following the instructions in [this doc](https://docs.anaconda.com/miniconda/miniconda-install/)).

We have prepared an `environment.yml` file that lists the dependencies and the versions of the dependencies. Once conda has been installed, create an environment from the .yml file by following the instructions in [this doc](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file).


### Downloading Data

For ease and efficiency of reproduction, we provide a simulator that replays request arrival traces on CPUs. The simulator implements all core logic in our system. Alongside the simulator, we also provide pickled data of requests in our workloads. Due to the size of these pickle files, we compress them (~435M) and host them on Google Drive and they can be downloaded via `gdown`.

```bash
mkdir apparate-ae; cd apparate-ae
pip install gdown  # only do this if the previous step was skipped
# download the tar file
gdown --fuzzy 'https://drive.google.com/file/d/1EN6ciNDBL2dEzSW4qdUTc9t4vOYkzWD8/view?usp=sharing'
# uncompress the tar file
tar -xzvf apparate-data.tar.gz && rm apparate-data.tar.gz
# clone this repo
git clone https://github.com/dywsjtu/apparate-ae.git
# create the directory for storing experiment output
mkdir apparate_latency
```


### Directory Structure

Once all the dependencies has been set up, the directory should have the following structure:

```
--apparate-ae
  --apparate-ae (this repo)
  --apparate_latency (empty, will be populated in the next step for plotting)
  --batch_decisions (downloaded from Google Drive and decompressed)
  --bootstrap_pickles (...)
  --optimal_latency
  --profile_pickles_bs
  --simulation_pickles
```

- `batch_decisions`: Contains the batching decisions of Clockwork using different models and request arrival traces. 
  - Format: `{model_name}_1_fixed_30.pickle` for 30FPS video traces (CV workloads), and `{model_name}_azure.pickle` for Microsoft Azure MAF traces (NLP workloads).
- `{bootstrap,simulation}_pickles`: Contains the confidence and accuracy of the bootstrapping/simulation dataset at all EE ramps.
  - Format: `{bootstrap,simulation}_{dataset}_{model_name}.pickle`. The pickled object `p` is a dict with two keys: "conf" and "acc". The confidence/accuracy of sample i at ramp r can be accessed via: `p["conf"/"acc"][r][i]`.
- `optimal_latency`: Contains the per-sample optimal latency for different workloads.
  - Format: `{model_name}_{dataset}_optimal.pickle`. The pickled object is a list of floats, with each one denoting the queuing delay + optimal model inference latency of a request.
- `profile_pickles_bs`: Contains the operator-level latency profile of different models at different batch sizes, all measured on an NVIDIA RTX A6000 GPU.
  - Format: `{model_name}_{batch_size}_profile.pickle` for vanilla models, and `{model_name}_{batch_size}_earlyexit_profile.pickle` for EE models.

## Reproducing Experiments

First, cd into the apparate-ae directory with the source code: `cd apparate-ae`.

To reproduce the CV main results in Fig. 12 and 13, run `python run_cv.py` (takes ~10-20 minutes on a 32-core CPU). To reproduce the NLP main results in Fig. 14, run `python run_nlp.py` (takes ~xxx minutes on a 32-core CPU).

Aggregate results can be found in `output_{cv,nlp}.txt`, which are generated by executing the above scripts. The system log, which details how our system performs ramp adjustment and threshold tuning, can be found in `/logs/output_{model_name}_{dataset}.log`.

Running the above scripts will also produce pickle files (`{model_name}_{dataset}_{arrival_trace}`) that contain detailed, per-request latencies in `/apparate_latency`.

## Plotting Results

- Fig. 12: Run `plot_cv_results_median.py` to plot the median latency wins (%) compared to vanilla inference. The output figure is named `cv_results_median.pdf`.
- Fig. 13: Run `plot_cv_results_p95.py` to plot the tail latency (ms) of Apparate and vanilla inference. The output figure is named `cv_results_p95.pdf`.
- Fig. 14: Run `plot_nlp_results.py` to plot the latency CDF of NLP workloads with and without Apparate. The output figures are named `nlp_results_{model_name}.pdf`.


