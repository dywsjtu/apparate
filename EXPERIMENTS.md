# SOSP 2024 Experiments

This document describes how to run the main experiments in the [SOSP '24](https://sigops.org/s/conferences/sosp/2024/) paper [Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving](https://arxiv.org/abs/2312.05385).

## Setup

### Hardware Setup

We have tested Apparate on CPU nodes on Cloudlab. We will provide more instructions on obtaining

### Downloading Data

For ease and efficiency of reproduction, we provide a simulator that replays request arrival traces on CPUs. The simulator implements all core logic in our system. Alongside the simulator, we also provide picked data of requests in our workloads. Due to the size of these pickle files, we compress them (~435M) and host them on Google Drive and they can be downloaded via utilities like `gdown`.

```bash
mkdir apparate-ae; cd apparate-ae
pip install gdown
# download the tar file
gdown --fuzzy 'https://drive.google.com/file/d/1EN6ciNDBL2dEzSW4qdUTc9t4vOYkzWD8/view?usp=sharing'
# uncompress the tar file
tar -xzvf apparate-data.tar.gz
rm apparate-data.tar.gz
# clone this repo
git clone https://github.com/dywsjtu/apparate-ae.git
```


### Software Dependencies

Prob just use conda?
Python can be installed using [Miniconda](https://docs.conda.io/en/latest/miniconda.html).

Required software dependencies can be installed using:

```bash
apt-get -y install cmake g++ gcc libnuma-dev make numactl zlib1g-dev
pip install -r scheduler/requirements.txt
cd scheduler; make
```

### Directory Structure

Once all the dependencies has been set up, the directory should have the following structure:

```
--apparate-ae
  --apparate-ae (this repo)
  --apparate_latency (this will be populated in the next step for plotting)
  --batch_decisions (downloaded from Google Drive and decompressed)
  --bootstrap_pickles (...)
  --optimal_latency
  --profile_pickles_bs
  --simulation_pickles
```

- `batch_decisions`: Contains the batching decisions of Clockwork using different models and request arrival traces. 
  - Format: `{model_name}_1_fixed_30.pickle` for 30FPS video traces (CV workloads), and `{model_name}_azure.pickle` for Microsoft Azure MAF traces (NLP workloads).
- `{bootstrap,simulation}_pickles`: Contains the confidence and accuracy of the bootstrapping/simulation dataset at all EE ramps.
  - Format: `{bootstrap,simulation}_{dataset}_{model_name}.pickle`. The pickled object `p` is a dict with two keys: "conf" and "acc". The confidence/accuracy of sample i at ramp r can be accessed via: `p["conf"/"acc"][r][i]`.
- `optimal_latency`: Contains the per-sample optimal latency for different workloads.
  - Format: `{model_name}_{dataset}_optimal.pickle`. The pickled object is a list of floats, with each one denoting the queuing delay + optimal model inference latency of a request.
- `profile_pickles_bs`: Contains the operator-level latency profile of different models at different batch sizes, all measured on an NVIDIA RTX A6000 GPU.
  - Format: `{model_name}_{batch_size}_profile.pickle` for vanilla models, and `{model_name}_{batch_size}_earlyexit_profile.pickle` for EE models.

## Reproducing Experiments

First, cd into the apparate-ae directory with the source code: `cd apparate-ae`.

To reproduce the CV main results in Fig. 12 and 13, run `python run_cv.py` (takes ~10-20 minutes on a 32-core CPU). To reproduce the NLP main results in Fig. 14, run `python run_nlp.py` (takes ~xxx minutes on a 32-core CPU).

Aggregate results can be found in output_{cv,nlp}.txt, which are generated by executing the above scripts. The system log, which details how our system performs ramp adjustment and threshold tuning, can be found in `/logs/output_{model_name}_{dataset}.log`.

Running the above scripts will also produce pickle files that contain detailed, per-request latencies in `/apparate_latency`.

## Plotting Results

TODO
